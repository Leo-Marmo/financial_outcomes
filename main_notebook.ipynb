{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45dd8380-af90-4d85-b61d-1bd5f7d01409",
   "metadata": {},
   "source": [
    "# **Financial Outcomes of Private Equity Portfolio Companies post-IPO** <a id='top'></a>\n",
    "\n",
    "## **Abstract**\n",
    "This paper investigates the long-term financial outcomes of companies that were formally private equity-backed by assessing the financial performance of these companies post-private equity exit. Private equity companies rely mostly on private-to-private sale, private-to-public acquisition, or IPO to exit from their investments. Since the two former exit strategies do not always require public disclosure of financials upon exit, I rely on companies that IPO'd from 2010 through 2018 to base my analysis on. The analysis is broken into two sections. The first analysis, which utilizes an OLS regression, compares the growth rates of company financial metrics to companies that IPO'd during the same time frame that were not private equity-backed. The second analysis is an event study that seeks to find if private equity-backed companies experience similar growth rates post-IPO as they were prior to exit.\n",
    "Below is all the source code for the two analyses that I run in the paper along with some of the outputs. \n",
    "\n",
    "\n",
    "**Click [here](#analysis_one) to jump to Analysis 1 <br>\n",
    "**Click [here](#analysis_two) to jump to Analysis 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c12a1-6279-4bb1-a94c-209b6d196567",
   "metadata": {},
   "source": [
    "<a id='data_cleaning'></a>\n",
    "## **Data Cleaning and Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5925cc03-6939-422f-8202-74c9062d705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns \n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from psmpy.plotting import *\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import stats\n",
    "import statsmodels.formula.api as smf\n",
    "from stargazer.stargazer import Stargazer\n",
    "from statistics import stdev\n",
    "import pingouin as pg\n",
    "import collections\n",
    "import statsmodels.api as sm\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "from IPython.display import display, Latex, Markdown\n",
    "pd.set_option('display.max_columns', 10)\n",
    "plt.rcParams['figure.figsize'] = [10, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f2823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Company Deal Info as well as company financials\n",
    "deal_info = pd.read_csv(r'./company_list.csv')\n",
    "company_fin = pd.read_csv(r'./company_financials_average_month_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02917ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all empty company financials with column median\n",
    "for col in company_fin.columns[2:]:\n",
    "    company_fin[col] = company_fin[col].apply(pd.to_numeric, errors='coerce')\n",
    "    company_fin[col] = company_fin[col].fillna(company_fin[col].median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c6730-97f3-4a97-969f-b089cf8bccc5",
   "metadata": {},
   "source": [
    "For data cleaning purposes, I fill all NaN values in the dataset with median values of each column. \n",
    "\n",
    "Some of the columns must also be converted into numeric types. When the csv files are read, the cell data is sometimes inferred as \"object\" types when they should be numeric types instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8597a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill deal_info columns with median values\n",
    "col = ['Year Founded', 'Revenue',\n",
    "       'Gross Profit', 'Net Income', 'EBITDA', 'EBIT', 'Total Debt',\n",
    "       'Fiscal Year', 'Valuation/Ebitda', 'Valuation/EBIT',\n",
    "       'Valuation/Net Income', 'Valuation/Revenue', 'Valuation / Cash Flow',\n",
    "       'Deal Size/EBITDA', 'Deal Size/EBIT', 'Deal Size/Net Income',\n",
    "       'Deal Size/Revenue', 'Deal Size/Cash Flow',\n",
    "       'Employees', 'Total Invested Equity','Deal Size', 'Deal Year']\n",
    "\n",
    "for c in col:\n",
    "    deal_info[c] = deal_info[c].apply(pd.to_numeric, errors='coerce')\n",
    "    deal_info[c] = deal_info[c].fillna(deal_info[c].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5ff012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cleaning all nan values and dropping first column of dataset\n",
    "cleaned_fin = company_fin.fillna(0).drop(index = 0)\n",
    "deal_info = deal_info.fillna(0)\n",
    "\n",
    "deal_info[\"EBITDA Margin %\"] = deal_info[\"EBITDA Margin %\"].str.replace(\"%\",\"\")\n",
    "\n",
    "deal_info[\"Company\"] = deal_info[\"Company\"].astype(str)\n",
    "deal_info[\"EBITDA Margin %\"] = deal_info[\"EBITDA Margin %\"].astype(float)\n",
    "deal_info[\"Employees\"] = deal_info[\"Employees\"].astype(int) \n",
    "deal_info[\"Total Invested Equity\"] = deal_info[\"Total Invested Equity\"].astype(float) \n",
    "deal_info = deal_info.fillna(0)\n",
    "\n",
    "\n",
    "cleaned_fin.rename(columns={\"Company Year\":\"Company\", '    Revenue % Growth':'revenue_growth', \n",
    "                            '    EBITDA % Growth':'ebitda_growth', 'Net Income Growth':'net_income_growth', \n",
    "                         'ROE % Growth':'roe_growth', 'Liquidity % Growth':'liquidity_growth'}, inplace = True)\n",
    "                         \n",
    "cleaned_fin[\"Company\"] = cleaned_fin[\"Company\"].astype(str)\n",
    "\n",
    "cleaned_fin.sort_values(\"Company\", inplace=True)\n",
    "cleaned_fin.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef81840f-6bbc-4b99-ba5a-8548fcb33c73",
   "metadata": {},
   "source": [
    "After that, we can calculate how far each financial statement was published from the time of IPO. This gives up information as to how the companies are doing per year in relation to IPO date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ebd155",
   "metadata": {},
   "outputs": [],
   "source": [
    "deal_info = deal_info[deal_info[\"Company\"].isin(cleaned_fin[\"Company\"])]\n",
    "cleaned_fin = cleaned_fin[cleaned_fin[\"Company\"].isin(deal_info[\"Company\"])]\n",
    "\n",
    "deal_info[deal_info[\"Primary Industry Sector\"] == \"Consumer Products and Services (B2C)\"]\n",
    "\n",
    "# keep track of how many years company has been around for\n",
    "years_around = []\n",
    "\n",
    "for fiscal,founded in zip(deal_info[\"Deal Year\"], deal_info[\"Year Founded\"]):\n",
    "    years = (fiscal - founded) if (fiscal > 0 and founded > 0) else 0\n",
    "    years_around.append(years)\n",
    "\n",
    "deal_info[\"Years Since Founding\"] = years_around\n",
    "deal_info[\"Years Since Founding\"] = deal_info[\"Years Since Founding\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f10b56-3750-49ce-93b9-202ef1547a82",
   "metadata": {},
   "source": [
    "Next we need to clean the data such that the quarterly financials are being used effectively. I lose around 20,000 rows by factoring out and just keeping the december quarterly trends. The issue with this is that by doing so, I am potentially losing out on best performance months for companies that may historically suffer during Q4 of the year. For instance, consumer companies would tend to do better when compared to, say banks, in winter months as consumer spending increases in the holidays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4933356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we label the PE-treated firms with binary labels\n",
    "def sum_and_avg_company_years(cleaned_fin, aggregate_mappings):\n",
    "    _agg_fin = cleaned_fin\n",
    "    display(cleaned_fin['Date of Financials'])\n",
    "    # First change all company financial months into their respective years\n",
    "    _agg_fin[\"Fin Year\"] = _agg_fin[\"Date of Financials\"].str.replace(r\"-\\w*\", \"\", regex=True).astype(int) + 2000\n",
    "\n",
    "    _rel_fin = cleaned_fin[cleaned_fin[\"Date of Financials\"].str.startswith(\"-Dec\")]\n",
    "    _rel_fin[\"Fin Year\"] = _rel_fin[\"Date of Financials\"].str.replace(\"-Dec\", \"\").astype(int) + 2000\n",
    "    _rel_fin = _rel_fin[['Company', 'Fin Year', 'revenue_growth',\n",
    "                         'ebitda_growth', 'net_income_growth', \n",
    "                         'roe_growth', 'liquidity_growth']]\n",
    "    \n",
    "        \n",
    "    #Group by relative financial year and then average the data to account for seasonality and lack of quarters\n",
    "    #For the time being, we will also keep growth estimates for only december growth patterns\n",
    "    _agg_fin = _agg_fin.groupby([\"Company\", \"Fin Year\"]).agg(aggregate_mappings).reset_index()\n",
    "\n",
    "    _agg_fin.merge(_rel_fin, on=['Company', 'Fin Year'])\n",
    "    return _agg_fin\n",
    "\n",
    "#Dictionary telling us which columns we should keep from the data that we have on hand\n",
    "agg = {'    Total Revenue':np.mean,\n",
    "       'revenue_growth':np.mean, \n",
    "       '    Gross Profit':np.mean,\n",
    "       '    Total Operating Profit/(Loss)':np.mean, \n",
    "       '    EBITDA':np.mean,\n",
    "       '    EBITDA (Normalized)':np.mean,\n",
    "       '    EBITDA Margin':np.mean, \n",
    "       'ebitda_growth':np.mean,\n",
    "       '    Net Income (Normalized)':np.mean,\n",
    "       'net_income_growth':np.mean,\n",
    "       '    Net Income (Analyst Normalized)':np.mean,\n",
    "       '    Diluted EPS':np.mean,\n",
    "       '    Diluted Weighted Average Shares Outstanding':np.mean,\n",
    "       '    Total Current Assets':np.mean, \n",
    "       '    Net Property, Plant and Equipment':np.mean,\n",
    "       '    Total Non-Current Assets':np.mean, \n",
    "       '    Total Assets':np.mean,\n",
    "       '    Total Current Liabilities':np.mean, \n",
    "       '    Total Non-Current Liabilities':np.mean,\n",
    "       '    Total Liabilities':np.mean, \n",
    "       '    Total Equity':np.mean,\n",
    "       'ROE':np.mean,  \n",
    "       'roe_growth':np.mean,\n",
    "       '    Equity Attributable to Parent Stockholders':np.mean,\n",
    "       '    Total Debt':np.mean,\n",
    "       '    Total Shares Outstanding (TSO)':np.mean,\n",
    "       '    Working Capital':np.mean,\n",
    "       '    Change in Cash':np.mean,\n",
    "       '    Capital Expenditure (Calc)':np.mean,\n",
    "       '    Issuance of/(Payments for) Common Stock, Net':np.mean,\n",
    "       '    Cash Dividends Paid':np.mean,\n",
    "       '    Cash and Cash Equivalents, Beginning of Period':np.mean,\n",
    "       '    Cash and Cash Equivalents, End of Period':np.mean,\n",
    "       'liquidity_growth':np.mean,\n",
    "       '    Net Income from Continuing Operations Sequential % Growth':np.mean,\n",
    "       '    Net Income Available to Common Stockholders Sequential % Growth':np.mean,\n",
    "       '    Current Ratio':np.mean,\n",
    "       '    Quick Ratio':np.mean,\n",
    "       '    Debt to Equity':np.mean,\n",
    "       '    Total Debt to Equity':np.mean,\n",
    "       '    Total Asset Turnover':np.mean,\n",
    "       '    Normalized Return on Equity':np.mean,\n",
    "       '    Normalized Return on Assets':np.mean,\n",
    "       '    Normalized Return on Invested Capital':np.mean,\n",
    "       '    Stock Price':np.mean,\n",
    "       '    Market Cap':np.mean,\n",
    "       '    EV':np.mean,\n",
    "       '    EV to Revenue':np.mean,\n",
    "       '    EV to EBIT':np.mean,\n",
    "       '    EV to EBIT (Normalized)':np.mean,\n",
    "       '    EV to EBIT (Analyst Normalized)':np.mean,\n",
    "       '    EV to EBITDA':np.mean,\n",
    "       '    EV to EBITDA (Normalized)':np.mean,\n",
    "       '    EV to EBITDA (Analyst Normalized)':np.mean,\n",
    "       '    Price to Earnings':np.mean,\n",
    "       '    Price to Earnings (Normalized)':np.mean,\n",
    "       '    Price to Earnings (Analyst Normalized)':np.mean,\n",
    "       '    Forward Price to Earnings':np.mean,\n",
    "       '    Price to Book (PB)':np.mean,\n",
    "       '    Price to Cash Flow (PCF)':np.mean,\n",
    "       '    Price to Tangible Book Value':np.mean}\n",
    "\n",
    "_rel_fin = sum_and_avg_company_years(cleaned_fin, agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ef3996-6e6f-44be-ac49-0f063fb9bf8a",
   "metadata": {},
   "source": [
    "Outliers that are either very high or very low in the dataset are also removed. Some of the companies in the dataset have growth values that are incredibly large (think 100000%) and as a result are heavily skewing the results of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db57dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    \n",
    "    if fence_low == np.nan or fence_high == np.nan or fence_high == fence_low:\n",
    "        return df_in\n",
    "    else:\n",
    "        df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "def rmv_all_outliers(df_list):\n",
    "    cleaned_list = []\n",
    "    for df in df_list:\n",
    "        for col in df.columns:\n",
    "            if is_numeric_dtype(df[col]):\n",
    "                df = remove_outlier(df, col)\n",
    "        cleaned_list.append(df)\n",
    "    return cleaned_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e94426-9dc7-47d3-9fa6-c119f2b2a26c",
   "metadata": {},
   "source": [
    "<a id='analysis_one'></a>\n",
    "## **Analysis 1 - Propensity Score Estimation**\n",
    "\n",
    "Through my analysis I investigate differences in revenue, EBITDA, net income, liquidity, and ROE growth rates of PE-treated and non-treated companies at one-, three-, and five-years after IPO. I find no statistical significance in revenue growth, net income growth, and ROE growth. However, I do find statistical significance in both liquidity and EBITDA growth. These results go in hand with the business model of private equity firms. Firms raise as much debt as possible when acquiring businesses, and thus the cash at hand (liquidity) that these companies will have to pay off debt is directly impacted by the amount of debt they take on. For each growth measure of growth tested, I compile my results in the form of an attached table and violin plot to demonstrate the distribution of data between treated and control groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b938960",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = [\"Primary Industry Sector\"]\n",
    "\n",
    "data_with_categ = pd.concat([deal_info.drop(columns=category),\n",
    "                            pd.get_dummies(deal_info[category], columns=category, drop_first=False)], axis=1)\n",
    "data_with_categ.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b7e90-7df6-4f8f-b491-3eed0d1c2192",
   "metadata": {},
   "source": [
    "Merging deal information with company financial information at IPO gives us more variables to use to calculate propensity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c319dcf-4858-44bc-86bc-18b304eb9b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_categ_merged = data_with_categ.merge(_rel_fin, left_on=['Company', 'Fiscal Year'], \n",
    "                                        right_on=['Company', 'Fin Year'], how=\"left\")\n",
    "data_with_categ_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5095e59b-8cab-4074-8898-1fc735d0b0d8",
   "metadata": {},
   "source": [
    "One of the one-hot-encode data columns are dropped since we are fitting the intercept.\n",
    "\n",
    "Propensity Scores are then calculated using the columns below with a Logistic Regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798fda09",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 'PE'\n",
    "\n",
    "X_axis = ['Revenue', 'Gross Profit', 'Net Income', 'EBITDA', 'EBIT', 'Total Debt',\n",
    "       'Valuation/Ebitda', 'Valuation/EBIT', 'Valuation/Net Income',\n",
    "       'Valuation/Revenue', 'Valuation / Cash Flow', 'EBITDA Margin %',\n",
    "       'Employees', 'Years Since Founding',\n",
    "       'Primary Industry Sector_Business Products and Services (B2B)',\n",
    "       'Primary Industry Sector_Consumer Products and Services (B2C)',\n",
    "       'Primary Industry Sector_Energy',\n",
    "       'Primary Industry Sector_Financial Services',\n",
    "       'Primary Industry Sector_Healthcare',\n",
    "       'Primary Industry Sector_Information Technology',]\n",
    "# 'Primary Industry Sector_Materials and Resources' column dropped\n",
    "ps_model = LogisticRegression(C=1e6, fit_intercept=True).fit(data_with_categ_merged[X_axis], data_with_categ_merged[T])\n",
    "data_ps = data_with_categ_merged.assign(propensity_score=ps_model.predict_proba(data_with_categ_merged[X_axis])[:, 1])\n",
    "# print(data_ps[\"propensity_score\"]) # print to show propensity scores calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f1bafd-9309-49b8-9aab-e6a8f13abda2",
   "metadata": {},
   "source": [
    "The covariance table below helps to show us that there is not obvious multicollinearity in the model. \n",
    "\n",
    "In other words, none of the covariances are close to +/-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b72e4b3-1590-4944-9397-416e9a45ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_ps[X_axis]\n",
    "df[X_axis[:11]].cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c99e50-7d43-4ce2-bb82-270ccf9a9e3d",
   "metadata": {},
   "source": [
    "As can be seen below, the weights of the data are not representative of the total sample size, but are importantly similar to each other. This would indicate that all least some of the confounders used to relatively compare portfolio and non-treated companies were well-captured in the model.\n",
    "\n",
    "After this, I graph the distribution of propensity scores for both groups of company data. What I get as a result is significant overlap between propensity scores of treated and non-treated companies. The distributions are also *relatively* centered close to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1142e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weights of the data\n",
    "weight_t = 1/data_ps.query(\"PE==1\")[\"propensity_score\"]\n",
    "weight_nt = 1/(1-data_ps.query(\"PE==0\")[\"propensity_score\"])\n",
    "print(\"Original sample size:\", deal_info.shape[0])\n",
    "print(\"Treated pop. sample size:\", sum(weight_t))\n",
    "print(\"Unreated pop. sample size:\", sum(weight_nt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123fc47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_ps = data_ps[data_ps[\"propensity_score\"] > 0.01]\n",
    "\n",
    "less_than_1 = data_ps.query(\"PE==1\")[\"propensity_score\"] \n",
    "\n",
    "less_than_0 = data_ps.query(\"PE==0\")[\"propensity_score\"] \n",
    "\n",
    "sns.distplot(less_than_0, kde=False, label=\"Non Treated\")\n",
    "sns.distplot(less_than_1, kde=False, label=\"Treated\")\n",
    "plt.legend()\n",
    "plt.title(\"Propensity Scores\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e5d12-afb8-47a9-a8a5-b3eb899e04e2",
   "metadata": {},
   "source": [
    "Once we calculate the propensity scores, we then append the scores to each of the company financials by mapping company name to propensity scores. \n",
    "\n",
    "Additionally, we also write in all the financial information if the financials came from a company with PE treatment or not.\n",
    "\n",
    "PE = 1 means the company is a portfolio company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e9b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_firms = deal_info.query(\"PE==1\")\n",
    "names = pe_firms['Company'].unique()\n",
    "treatment = []\n",
    "for val in _rel_fin['Company']:\n",
    "    t = 1 if val in names else 0\n",
    "    treatment.append(t)\n",
    "_rel_fin[\"PE\"] = treatment\n",
    "# _rel_fin #print to show with new PE column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523fbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary of prop scores mapping name to score\n",
    "_rel_fin['propensity_score'] = _rel_fin['Company']\n",
    "\n",
    "new_map = {}\n",
    "for index, deal in data_ps.iterrows():\n",
    "        comp = deal[\"Company\"]\n",
    "        if comp not in new_map:\n",
    "            new_map[comp] = deal[\"propensity_score\"]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b4123",
   "metadata": {},
   "outputs": [],
   "source": [
    "_rel_fin[\"propensity_score\"] = _rel_fin[\"propensity_score\"].replace(new_map)\n",
    "_rel_fin = _rel_fin[_rel_fin[\"Company\"].isin(new_map.keys())]\n",
    "\n",
    "_rel_fin[\"propensity_score\"] = _rel_fin[\"propensity_score\"].astype(float)\n",
    "# _rel_fin #print to show with new propensity score mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0584fb2b-faa7-4709-93af-7328f9085625",
   "metadata": {},
   "source": [
    "The functions below are useful to determine the year of a company financial and how many years before or after the IPO it was published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bfd31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines function to return relevant financial of the company per year\n",
    "def fin_by_year(df, company_name, year_after_deal):\n",
    "    year = df[df['Company'] == company_name]['Deal Year'].values[0] + year_after_deal\n",
    "    return _rel_fin[(_rel_fin['Company'] == company_name) & (_rel_fin['Fin Year'] == year)]\n",
    "\n",
    "def x_years_out(df, years):\n",
    "    names = df['Company'].unique()\n",
    "    df_orig = df\n",
    "    x_year_data = []\n",
    "    for company in names:\n",
    "        df = fin_by_year(df_orig, company, years)\n",
    "        x_year_data.append(df)\n",
    "    df_out = pd.concat(x_year_data)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d960642-a6f6-468d-bcca-f5f1bc445282",
   "metadata": {},
   "source": [
    "By knowing how many years out our financials are from the date of IPO, we can infer the results of our model for the years that we care about.\n",
    "                                                                                                            \n",
    "In the model, years 1, 3, and 5 are the years of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_results = []\n",
    "for years in [1,3,5]:\n",
    "    x_year = x_years_out(data_ps, years)\n",
    "    data_results.append(x_year)\n",
    "\n",
    "# Making a function that does the same thing below to not mess up anything\n",
    "def data_time_horizon(dataset, list_of_years):\n",
    "    data_results = []\n",
    "    for years in list_of_years:\n",
    "        x_year = x_years_out(dataset, years)\n",
    "        data_results.append(x_year)\n",
    "    return data_results\n",
    "    \n",
    "# data_results[0] #uncomment to show financials from year 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb457c9-76f8-4482-b827-02d4e644d393",
   "metadata": {},
   "source": [
    "Now we can build an OLS model for each of the years and for each of the growth metrics of interest.\n",
    "\n",
    "**Y** in this case are the variables that we want to analyze.\n",
    "\n",
    "**X** in this model are the PE treatment columns and the propensity scores that we calculated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77464acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in data_results:\n",
    "    var_to_analyze = 'liquidity_growth'\n",
    "    df = remove_outlier(df, var_to_analyze)\n",
    "    Y = df[var_to_analyze].values #scale the variables\n",
    "    X = df[[\"PE\",\"propensity_score\"]].values\n",
    "\n",
    "    model = sm.OLS(Y,X)\n",
    "    results = model.fit()\n",
    "    display(results.params)\n",
    "    print(results.t_test([1, 0]))\n",
    "    print(\"Number of Observations: \", Y.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b62d09c-08e9-4664-8da1-a3a4f299a686",
   "metadata": {},
   "source": [
    "Now that the model works, we can build one for each of the years and each of the variables of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab8a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_analyze = ['revenue_growth','ebitda_growth','net_income_growth','liquidity_growth','roe_growth']\n",
    "for var_to_analyze in vars_to_analyze:\n",
    "    models = []\n",
    "    t_tests = []\n",
    "    year = 1;\n",
    "    for df in data_results:\n",
    "        df = remove_outlier(df, var_to_analyze)\n",
    "        Y = df[var_to_analyze]\n",
    "        X = df[[\"PE\",\"propensity_score\"]]\n",
    "        X['Y'] = Y\n",
    "        mod = smf.ols('Y ~ propensity_score + PE', data=X)\n",
    "        res = mod.fit()\n",
    "        t_test = res.t_test('PE = 0')\n",
    "        print(f\"\\nYear {year} results for {var_to_analyze}\")\n",
    "        display(t_test)\n",
    "        t_tests.append(t_test)\n",
    "        models.append(res)\n",
    "    # stargazer = Stargazer(models)             #USED TO OUTPUT THE MODEL IN LATEX FORMAT\n",
    "    # display(Latex(stargazer.render_latex()))  #USED TO OUTPUT THE MODEL IN LATEX FORMAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b6de14-2189-486a-b057-7c0811bfc7f9",
   "metadata": {},
   "source": [
    "Output of the growth estimates is also graphed to see the overall distributions of both groups. \n",
    "\n",
    "Comment out the variables that you don't want to run and add variables that you do want to see distributions for in the vars_to_analyze list. Please only run one variable at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00123a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out and uncomment the variables that you'd like to display to compare the against the two \n",
    "# Please run only one variable at a time\n",
    "vars_to_analyze = ['liquidity_growth'] #, 'roe_growth','revenue_growth','ebitda_growth','net_income_growth','liquidity_growth']\n",
    "\n",
    "# Outputs a violin plot for both treated and non-treated companies per variable\n",
    "# passed into to model\n",
    "def show_var_distributions(vars_to_analyze:list[str], data_results:pd.DataFrame) -> None:\n",
    "    pe = []\n",
    "    no_pe = []\n",
    "    for var_to_analyze in vars_to_analyze:\n",
    "        count = 0\n",
    "        for i in [1,3,5]:\n",
    "            df = data_results[count]\n",
    "            df = remove_outlier(df, var_to_analyze)\n",
    "            Y = df[var_to_analyze] * 100\n",
    "            X = df[[\"PE\",\"propensity_score\"]]\n",
    "            X['Y'] = Y\n",
    "            x_1 = X.query(\"PE==1\")['Y']\n",
    "            x_1 = x_1[x_1 > 0]\n",
    "            x_1 = (x_1.values)\n",
    "            \n",
    "            x_pe = x_npe = np.full(len(x_1), 1, dtype=str)\n",
    "            arr = np.full(len(x_1), i, dtype=str)\n",
    "            data = {\"Year\":arr,\"Value\":x_1,\"PE\":x_pe}\n",
    "            \n",
    "            x_0 = X.query(\"PE==0\")['Y']\n",
    "            x_0 = x_0[x_0 > 0]\n",
    "            x_0 = (x_0.values)\n",
    "            x_npe = np.full(len(x_0), 0, dtype=str)\n",
    "            arr_o = np.full(len(x_0), i, dtype=str)\n",
    "            dato = {\"Year\":arr_o,\"Value\":x_0,\"PE\":x_npe}\n",
    "            \n",
    "            do = pd.DataFrame(dato)\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            pe.append(pd.concat([df,do], axis=0))\n",
    "            \n",
    "            count +=1\n",
    "    df = pd.concat(pe, axis=0)\n",
    "    sns.set(rc={'figure.figsize':(11.7,8.27)}, font_scale=2)\n",
    "    ax = sns.violinplot(df, x = \"Year\", y=\"Value\", hue='PE', split=True)\n",
    "    ax.set(ylabel='Value (%)')\n",
    "    ax.axhline(0, ls='--')\n",
    "    plt.show()\n",
    "\n",
    "show_var_distributions(vars_to_analyze, data_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cdcbaf-2b4e-4083-917e-ecc04ab87600",
   "metadata": {},
   "source": [
    "<a id='analysis_two'></a>\n",
    "## **Analysis 2 - Financial Performance Event Study**\n",
    "\n",
    "Please visit [this link](https://libguides.princeton.edu/eventstudy) from Princeton University for a detailed explanation of the model using Stata.<br>\n",
    "[https://libguides.princeton.edu/eventstudy](https://libguides.princeton.edu/eventstudy)\n",
    "\n",
    "### **Data Preparation and Calculating the Event Window**\n",
    "Cleaning the Data was pretty straightforward. Because I already had methods to extract rows of company financials based on the amount of years from IPO, all I really had to do in this step was to create a new data table for each company ranging from three years before IPO to five years after. The amount of datapoints ranges from company to company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72deab06-0ffe-4feb-a58e-d76c3726923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CALCULATING THE EVENT WINDOW\n",
    "\n",
    "# break down information on companies on a yearly basis trusting for the moment that this works\n",
    "COMPANY_FINANCIALS_BY_IPO_DELTA = data_time_horizon(data_ps, [x for x in range(-3, 6)])\n",
    "\n",
    "for i in range(len(COMPANY_FINANCIALS_BY_IPO_DELTA)):\n",
    "    frame = COMPANY_FINANCIALS_BY_IPO_DELTA[i]\n",
    "    frame = frame.query(\"PE==1\")\n",
    "    COMPANY_FINANCIALS_BY_IPO_DELTA[i] = frame\n",
    "\n",
    "#Extracts company data from the COMPANY_FINANCIALS_BY_IPO_DELTA table\n",
    "def get_company_deltas(company_name:str):\n",
    "    first_entry = COMPANY_FINANCIALS_BY_IPO_DELTA[0]\n",
    "    company_deltas = first_entry[first_entry[\"Company\"] == company_name]\n",
    "    for set in COMPANY_FINANCIALS_BY_IPO_DELTA[1:]:\n",
    "        year_data = set[set[\"Company\"] == company_name]\n",
    "        company_deltas = pd.concat([company_deltas, year_data], axis=0)\n",
    "    return company_deltas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f25bf5-cbbd-4850-be8d-c83074879dfb",
   "metadata": {},
   "source": [
    "To verify that the model works, I first test the analysis one one company in particular.\n",
    "\n",
    "The company, GoDaddy, was chosen at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737426c5-8679-478f-a9bd-968d71556288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to show that it works for 1 company -- to be extracted later\n",
    "go_daddy_data = get_company_deltas(\"GoDaddy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d3eb06-cef1-4d02-bdd2-cbd0041f5dfd",
   "metadata": {},
   "source": [
    "### **Estimating Normal Performance**\n",
    "\n",
    "To initially build the models, I first find the estimation (training data) and event (testing data) windows for each of the companies tested. For each of the companies, I train a new model for each of the growth variables that are tested.\n",
    "\n",
    "**estimation window**: financial data for the company from years -3 to 0 from IPO <br>\n",
    "**event window**: financial data for the company from years 1 to 5 from IPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd4c496-57c3-4896-8480-1c44070f385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a boolean to see if the specified financial year is in the event window\n",
    "def in_event_window(company, row):\n",
    "    return data_ps[data_ps['Company'] == company][\"Deal Year\"] < row['Fin Year']\n",
    "\n",
    "#Returns a boolean to see if the specified financial year is in the estimation window\n",
    "def in_estimation_window(company, row):\n",
    "    return ~in_event_window(company, row)\n",
    "\n",
    "#Later to be abstracted to work for all companies\n",
    "#Returns a DataFrame of company financials that are in the estimation window\n",
    "def estimation_window(company_data:pd.DataFrame)->pd.DataFrame:\n",
    "    return company_data.apply(lambda row:in_event_window('GoDaddy', row), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53748422-8cf4-4fcb-9e54-15968cd0ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Years less than or equal to the IPO. These will be what use to calculate normal performance\n",
    "estimation_window = go_daddy_data.apply(lambda row: in_estimation_window('GoDaddy', row), axis=1).iloc[:,0]\n",
    "estimates = go_daddy_data[estimation_window]\n",
    "\n",
    "# Financials for all years after the IPO that we want to analyze the normal performance with\n",
    "event_window = estimation_window = go_daddy_data.apply(lambda row: in_event_window('GoDaddy', row), axis=1).iloc[:,0]\n",
    "events = go_daddy_data[event_window]\n",
    "\n",
    "def estimation_and_event_windows(company_data, company):\n",
    "    estimation_window = company_data.apply(lambda row: in_estimation_window(company, row), axis=1).iloc[:,0]\n",
    "    estimates = company_data[estimation_window]\n",
    "\n",
    "    event_window = estimation_window = company_data.apply(lambda row: in_event_window(company, row), axis=1).iloc[:,0]\n",
    "    events = company_data[event_window]\n",
    "\n",
    "    return estimates, events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc93638-273e-41c8-9649-37c5d7c503f1",
   "metadata": {},
   "source": [
    "Once we have the proper tuples of data, we can now run a regression on the estimation window to see what normal performance was and then save the alphas. For now, using dummy cols which I think could be roughly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97547fdc-fd99-4e72-890e-6c5ca352764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGRESSOR_COLS = ['    Gross Profit', '    Diluted EPS', '    Cash Dividends Paid', '    Debt to Equity', '    Stock Price', '    Price to Tangible Book Value']\n",
    "regressors = estimates[REGRESSOR_COLS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09376b4f-102d-4ecf-851f-96ee515cddf4",
   "metadata": {},
   "source": [
    "<b/>Now I fit the model to EBITDA Growth to see how it is impacted</b>\n",
    "\n",
    "EBITDA Growth is used an example here to make sure that the models works for GoDaddy. The analysis is later generalized to work for all of the companies and growth metrics tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b18b76-9ddb-4219-b247-9ea10a4d50ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sm OLS models for each of the variables that I intend to analyze\n",
    "# The OLS models are contained in a dictionary mapped to the analyzed variable's name\n",
    "vars_to_analyze = ['roe_growth','revenue_growth','ebitda_growth','net_income_growth','liquidity_growth']\n",
    "\n",
    "# Returns a dictionary of models for the company estimates passed in\n",
    "def create_company_models(estimates, regressors, vars_to_analyze):\n",
    "    models = {}\n",
    "    \n",
    "    for var in vars_to_analyze:\n",
    "        growth = estimates[var]\n",
    "        Y = growth\n",
    "        # display(regressors)\n",
    "        X = sm.add_constant(regressors, has_constant='add')\n",
    "        # display(X.columns)\n",
    "        model = sm.OLS(Y, X)\n",
    "        results = model.fit()\n",
    "        models[var] = results\n",
    "    return models\n",
    "\n",
    "go_daddy_models = create_company_models(estimates, regressors, vars_to_analyze)\n",
    "go_daddy_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc0268-dfdf-48ef-8bde-e1520f294575",
   "metadata": {},
   "source": [
    "### **Testing for Abnormal Returns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7403cc04-714a-423f-b6b1-3c4ba908da90",
   "metadata": {},
   "source": [
    "Now we are able to test for significance in the testing window. To do this, we must add up the difference between what the model predicted and what the actual growth was for the given year. After the cumulative aggregate growth of the model is found, we can build a simple t-tests to test how closely the difference for each of the years for each of the models is close to 0. By doing so, we can see how far the companies deviate after IPO from the growth rates that were expected of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb53ce-ca3f-49c2-adbc-9fb91ddb692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_abnormal_return(models, events, vars_to_analyze, regressor_cols):\n",
    "    abn_returns = {}\n",
    "    reg = regressor_cols[:]\n",
    "    reg.extend(vars_to_analyze)\n",
    "    event = events[reg]\n",
    "    reg = ['const'] + regressor_cols[:] \n",
    "    for var in vars_to_analyze:\n",
    "        abn_list = []\n",
    "        if var in models:\n",
    "            sample = sm.add_constant(event, has_constant='add')\n",
    "            abn = models.get(var).predict(sample[reg])\n",
    "            abn = event[var] - abn\n",
    "            abn_returns[var] = abn.tolist()\n",
    "    return abn_returns\n",
    "\n",
    "all_abnormal_returns = cumulative_abnormal_return(go_daddy_models, events, vars_to_analyze, REGRESSOR_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53f4ec-e322-4494-a6bf-962abf7166a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_sig_test_estimates(predicted_var, abnormal_returns):\n",
    "    simple_sig_test = (sum(abnormal_returns)/len(abnormal_returns))/(stdev(abnormal_returns)/(len(abnormal_returns)**.5))\n",
    "    return simple_sig_test\n",
    "\n",
    "def compute_company_estimates(all_abnormal_returns):\n",
    "    estimates = {}\n",
    "    for k, v in all_abnormal_returns.items():\n",
    "        if (len(v) > 2) and not all([x==0 for x in v]):\n",
    "            estimates[k] = simple_sig_test_estimates(k, v)\n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71654311-2095-43f3-92ac-03609d0f212c",
   "metadata": {},
   "source": [
    "### **Generalizing Framework to Work for Multiple Companies**\n",
    "\n",
    "After testing the model on GoDaddy, we can abstract the model to work for all of the other portfolio companies that should be tested. Below is a dataframe describing the columns within the dataset that will be used to construct the models for each of the companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e41fe6-4cd7-495f-8133-c81a2db98e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPANY_FINANCIALS_BY_IPO_DELTA: variable includes financials of all companies and orders them on the -3 to 5 year time horizon specified\n",
    "#Dataset for the time being does not actually include information on industry data\n",
    "company_industry_data = deal_info[[\"Company\", \"Primary Industry Sector\"]]\n",
    "industry_map = {}\n",
    "financials_with_industry_data = []\n",
    "for _, row in company_industry_data.iterrows():\n",
    "    if row['Company'] not in industry_map:\n",
    "        industry_map[row['Company']] = row[\"Primary Industry Sector\"]\n",
    "\n",
    "# populates the dataset with Industry data so I can break down my results based on that\n",
    "for df in COMPANY_FINANCIALS_BY_IPO_DELTA:\n",
    "    df['Industry'] = df['Company']\n",
    "    df['Industry'] = df['Industry'].replace(industry_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40577a60-e184-4174-863a-4d3f24b68226",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_FINANCIALS_BY_IPO_DELTA[0].columns\n",
    "# company_industry_data\n",
    "COMPANY_FINANCIALS_BY_IPO_DELTA[0].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e2e34d-c935-48cf-b03a-981e1e368b64",
   "metadata": {},
   "source": [
    "Get event study estimate takes in a list of companies and does 2 things\n",
    "\n",
    "1) It calculates t-statistics for each of the companies given their cumulative abnormal returns\n",
    "2) maps the calculated statistics for each of the growth metrics to the company name that produced the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53474b5e-130f-4620-81bf-3d02dd1c9f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all available significance for the companies that there is enough information for\n",
    "# Also provides mapping of company significance to outlier data ASSUMES THAT NO SIGNIFICANCE IS EXACTLY EQUAL TO ANOTHER\n",
    "companies_with_sufficient_data = COMPANY_FINANCIALS_BY_IPO_DELTA[0]['Company'].unique()\n",
    "REGRESSOR_COLS = ['    Gross Profit', '    Diluted EPS', '    Cash Dividends Paid', '    Debt to Equity', '    Stock Price', '    Price to Tangible Book Value']\n",
    "vars_to_analyze = ['roe_growth','revenue_growth','ebitda_growth','net_income_growth','liquidity_growth']\n",
    "def get_event_study_estimates(companies:list[str]):\n",
    "    company_significance = []\n",
    "    mp = collections.defaultdict(dict)\n",
    "\n",
    "    def add_to_mp(company, sig):\n",
    "        for k, v in sig.items():\n",
    "            mp[k][v] = company\n",
    "    \n",
    "    for c in companies:\n",
    "        company_deltas = get_company_deltas(c)\n",
    "        estimates, events = estimation_and_event_windows(company_deltas, c)\n",
    "        models = create_company_models(estimates, estimates[REGRESSOR_COLS], vars_to_analyze)\n",
    "        abnormal_returns = cumulative_abnormal_return(models, events, vars_to_analyze, REGRESSOR_COLS)\n",
    "        significance = compute_company_estimates(abnormal_returns)\n",
    "        \n",
    "        company_significance.append(significance)\n",
    "        add_to_mp(c, significance)\n",
    "\n",
    "    return mp, company_significance\n",
    "\n",
    "company_sig_map, all_company_signicance = get_event_study_estimates(companies_with_sufficient_data)\n",
    "# display(company_sig_map) # display the map that maps t-stats per growth metric to the company that produced it\n",
    "# display(all_company_signicance) # display the map that shows a list of al computer t-stats for each of the growth metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2150f7d-ff6d-4c58-845c-054568ee50a6",
   "metadata": {},
   "source": [
    "### **Understanding the Analysis**\n",
    "At this point we have data for the companies tested to see if their growth was able to get sustained into their IPO. Using the data, we can graph the distributions of all the outputted t-tests for the companies and run a separate t-test for each of the growth metrics we want to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f054d68-7fe7-40b8-b665-c578ba534b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First thing is to compile each one of the sigificance variables into their proper lists to compare\n",
    "\n",
    "def list_of_significances(significances):\n",
    "    mp = collections.defaultdict(list)\n",
    "    for m in significances:\n",
    "        for k, v in m.items():\n",
    "            mp[k] += [v]\n",
    "    return mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d36ef-89b2-4a7b-a7f3-837a82caede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we have the proper lists, we can then plot histograms for each element\n",
    "def plot_histograms(mapped_significance):\n",
    "    for k, v in mapped_significance.items():\n",
    "        hist = sns.histplot(x=v)\n",
    "        hist.set(xlabel=(k + \" t-test plot\"))\n",
    "        plt.show()\n",
    "\n",
    "plot_histograms(significance_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78b67e6-8e25-41a2-8e1a-72721b72a20c",
   "metadata": {},
   "source": [
    "### **Rerunning the T-Test on Compiled Company Significances**\n",
    "\n",
    "Lastly, t-statistics for each of the companies is compiled into another t-test so that we may generalize the results of the analysis for portfolio companies as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f271981b-beea-4ea7-b5ec-5502a92b2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_compiled_ttest(key_map):\n",
    "    for k, v in key_map.items():\n",
    "        X = v\n",
    "        Y = 0\n",
    "        print(f\"{k} two-sided ttest\")\n",
    "        display(pg.ttest(X, Y))\n",
    "\n",
    "display_compiled_ttest(significance_map)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad61c12-7a79-4b65-a9f3-50238fce988f",
   "metadata": {},
   "source": [
    "### **Finding Outliers in the Models and Retesting**\n",
    "\n",
    "Lastly, we investigate the outliers that are apparent in the event study. The analysis sees two in particular with the data set that was tested (namely The Habit Burger and K2M).\n",
    "\n",
    "To see what is happening, I look into the event and estimation directly and see what the data looks like. Then I retest the model with updated covariates to see if the significances remain the same. As can be seen, the t-stats decrease drastically.\n",
    "\n",
    "Please uncomment the function call lines one at a time to see how the outliers are found and how their models change after we slightly modify the training data to use less regressor columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abba5f21-9837-42f1-8723-5219aa2f12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the list of all outliers for each growth metric given the list of t-statistics\n",
    "def get_outliers(significances:dict, company_mappings:dict)->dict:\n",
    "    mp = collections.defaultdict(list)\n",
    "    def calculate_outliers(data):\n",
    "        _75th = np.percentile(data,75)\n",
    "        _25th = np.percentile(data, 25)\n",
    "        iqr = _75th - _25th\n",
    "        # print(_75th, _25th)\n",
    "\n",
    "        return _25th - (1.5*iqr), _75th + (1.5*iqr)\n",
    "\n",
    "    for k, v in significances.items():\n",
    "        lower, upper = calculate_outliers(v)\n",
    "\n",
    "        outliers = np.array([[company_mappings[k][x], round(x,3)] for x in v if (x > upper or x < lower)])\n",
    "        mp[k] = outliers\n",
    "    return mp\n",
    "\n",
    "\n",
    "significance_map = list_of_significances(all_company_signicance)\n",
    "outliers = get_outliers(significance_map, company_sig_map) \n",
    "\n",
    "\n",
    "# Prints out the outliers per growth metric and their t-statistics\n",
    "def print_outliers(outliers):\n",
    "    for k, v in outliers.items():\n",
    "        companies = v[0:, 0]\n",
    "        sigs = v[0:,1]\n",
    "        dict = {'companies': companies, 'significance': sigs}\n",
    "        \n",
    "        df = pd.DataFrame(data=dict)\n",
    "        print(k)\n",
    "        display(df)\n",
    "        \n",
    "# print_outliers(outliers) # Run to print the full list of outliers in the model\n",
    "\n",
    "\n",
    "# Prints the event and estimation window of the outlier companies\n",
    "def print_outlier_financials(companies:list[str])->None: \n",
    "    REGRESSOR_COLS = ['Company', '    Gross Profit', '    Diluted EPS', '    Cash Dividends Paid', \n",
    "                      '    Debt to Equity', '    Stock Price', '    Price to Tangible Book Value']\n",
    "    \n",
    "    for c in companies:\n",
    "        df = get_company_deltas(c)\n",
    "        est, ev = estimation_and_event_windows(df, c)\n",
    "        display(est[REGRESSOR_COLS])\n",
    "        display(ev[REGRESSOR_COLS])\n",
    "\n",
    "\n",
    "# print_outlier_financials(companies) # Run to print the financial data of the outlier companies\n",
    "\n",
    "\n",
    "REGRESSOR_COLS_RETEST = ['    Gross Profit', '    Diluted EPS', '    Stock Price', '    Price to Tangible Book Value']\n",
    "var_to_analyze = ['roe_growth','revenue_growth','ebitda_growth','net_income_growth','liquidity_growth']\n",
    "companies = ['The Habit Burger Grill', 'K2M']\n",
    "\n",
    "\n",
    "# Recaulculates the event study estimates using updated regressors\n",
    "def get_event_study_estimate_for_outliers(companies:list[str], REGRESSOR_COLS_RETEST:list[str])->list[float]:\n",
    "    company_significance = []\n",
    "    \n",
    "    for c in companies:\n",
    "        company_deltas = get_company_deltas(c)\n",
    "        estimates, events = estimation_and_event_windows(company_deltas, c)\n",
    "        display(estimates[REGRESSOR_COLS_RETEST])\n",
    "        models = create_company_models(estimates, estimates[REGRESSOR_COLS_RETEST], var_to_analyze)\n",
    "        abnormal_returns = cumulative_abnormal_return(models, events, var_to_analyze, REGRESSOR_COLS_RETEST)\n",
    "        display(abnormal_returns)\n",
    "        significance = compute_company_estimates(abnormal_returns)\n",
    "        \n",
    "        company_significance.append(significance)\n",
    "    \n",
    "    return company_significance\n",
    "\n",
    "company_estimates = get_event_study_estimate_for_outliers(companies, REGRESSOR_COLS_RETEST)\n",
    "display(company_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d7ca29-7419-498e-97d9-a70db47dc945",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Click [here](#top) to jump to the top of the notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
